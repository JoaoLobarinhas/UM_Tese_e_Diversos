import re
import networkx as nx
import community
from collections import defaultdict
import collections
import csv
import sys
import operator
import datetime
import math
import scipy.stats as stats

tweet_id = sys.argv[1]

start_program = datetime.datetime.now()
print('Running for: ', tweet_id)
print('Program started: '+str(start_program))


# Spreader file
retweet_file = ''.join(['retweets_', tweet_id, '.txt'])
s_spreaders = set()
with open(retweet_file) as infile:
    for line in infile:
        l_spl = re.split(r'[,]', line.rstrip())
        s_spreaders.add(l_spl[1])
    
# TS file
ts_file = ''.join(['TS_network_', tweet_id, '.txt'])
d_TS = {}
with open(ts_file) as infile:
    for line in infile:
        l_spl = re.split(r'[ ]', line.rstrip())
        d_TS[l_spl[0]] = [l_spl[1], l_spl[2]]

print('reading graph...')
network_file = ''.join(['network_', tweet_id, '.txt'])
G = nx.DiGraph()
with open(network_file) as infile:
    for line in infile:
        l_spl = re.split(r'[,]', line.rstrip())
        G.add_edge(l_spl[0], l_spl[1])

metrics_file = ''.join(['METRICS_', tweet_id, '.txt'])
with open(metrics_file, 'w') as f:
    print('Node count: ', len(G.nodes))
    print('Edge count: ', len(G.edges))
    print('Node Density: ', len(G.edges) / float(len(G.nodes) * (len(G.nodes) - 1)))
    f.write('Node count: ' + str(len(G.nodes)) + '\n')
    f.write('Edge count: ' + str(len(G.edges)) + '\n')
    f.write('Node Density: ' + str(len(G.edges) / float(len(G.nodes) * (len(G.nodes) - 1))) + '\n')
    f.write('No of infected nodes: ' + str(len(s_spreaders)) + '\n')


def read_cd_file(cd_file, cd):
    print('Reading communities from ', cd_file)
    clusters = {}
    with open(cd_file) as infile:
        i = 0
        for line in infile:
            l_spl = re.split(r'[,]', line.rstrip())
            node_set = set(l_spl)
            clusters[i] = node_set
            i+=1
    with open(metrics_file, 'a') as f:
        f.write('Metrics for: '+ cd +'\n')
    return clusters


def run_algorithm(G, s_spreaders, cluster):
    d_vul_b = collections.defaultdict(dict)
    d_vul_C = {}
    graph_nodes = set(G.nodes())
    d_vul_b_GT = collections.defaultdict(set)  # All infected boundary nodes of the network.
    d_vul_C_GT = {}  # Dict storing no. of infected nodes in each community.
    cl_nodes = []
    cl_infected_nodes = []
    cl_boundary_edges = []
    cl_boundary_nodes = []
    cl_neighbor_nodes = []
    cl_boundary_nodes_inf = []
    cl_neighbor_nodes_inf = []
    print('No. of clusters:', len(cluster.keys()))
    for cl in cluster:
        cluster_nodes = set(cluster[cl])
        cl_nodes.append(len(cluster_nodes))
        infected_nodes = s_spreaders.intersection(cluster_nodes)
        cl_infected_nodes.append(len(infected_nodes))
        boundary_edges = list(nx.edge_boundary(G, graph_nodes.difference(cluster_nodes), cluster_nodes))
        cl_boundary_edges.append(len(boundary_edges))
        boundary_nodes = set([i[1] for i in boundary_edges])
        cl_boundary_nodes.append(len(boundary_nodes))
        neighbor_nodes = set([i[0] for i in boundary_edges])
        cl_neighbor_nodes.append(len(neighbor_nodes))
        cl_boundary_nodes_inf.append(len(s_spreaders.intersection(boundary_nodes)))
        cl_neighbor_nodes_inf.append(len(s_spreaders.intersection(neighbor_nodes)))
        boundary_neighbor_dict = defaultdict(set)
        for edge in boundary_edges:
            boundary_neighbor_dict[edge[1]].add(edge[0])

        var1 = 1
        for b in boundary_nodes:
            N_b = boundary_neighbor_dict[b]
            var2 = 1
            for n in N_b:
                aux = d_TS.get(n)
                aux2= d_TS.get(b)
                var2 = var2 * (1 - (float(aux[1]) * float(aux2[0])))
            vul_b = 1 - var2
            d_vul_b[cl][b] = vul_b
            var1 = var1 * (1 - d_vul_b[cl][b])
            if b in infected_nodes:
                d_vul_b_GT[cl].add(b)
        d_vul_C[cl] = 1 - var1
        # d_vul_C_GT[cl] = len(d_vul_b_GT[cl])
        d_vul_C_GT[cl] = len(d_vul_b_GT[cl])/float(len(boundary_nodes)+1)

    with open(metrics_file, 'a') as f:
        f.write('Avg. no. of nodes in C = ' + str(sum(cl_nodes) / float(len(cl_nodes))) + '\n')
        f.write(
            'Avg. no. of infected nodes in C = ' + str(sum(cl_infected_nodes) / float(len(cl_infected_nodes))) + '\n')
        f.write('Avg. no. of incoming boundary edges = ' + str(
            sum(cl_boundary_edges) / float(len(cl_boundary_edges))) + '\n')
        f.write('Avg. no. of boundary nodes = ' + str(sum(cl_boundary_nodes) / float(len(cl_boundary_nodes))) + '\n')
        f.write('Avg. no. of neighbor nodes = ' + str(sum(cl_neighbor_nodes) / float(len(cl_neighbor_nodes))) + '\n')
        f.write('Avg. no. of infected boundary nodes = ' + str(
            sum(cl_boundary_nodes_inf) / float(len(cl_boundary_nodes_inf))) + '\n')
        f.write('Avg. no. of infected neighbor nodes = ' + str(
            sum(cl_neighbor_nodes_inf) / float(len(cl_neighbor_nodes_inf))) + '\n')

    return (d_vul_b, d_vul_b_GT, d_vul_C, d_vul_C_GT)


def boundary_metric_evaluation(d_vul_b, d_vul_b_GT, clusters):
    cl_vul_b_precision = {}
    cl_vul_b_avg_precision = {}
    for cl in clusters:
        print('for cluster: ', cl)
        cl_vul_dict = d_vul_b[cl]
        #     rel_list = d_vul_b_GT[cl].intersection(set(cl_vul_dict.keys())) #Set of boundary nodes in the cluster that are infected.
        rel_list = d_vul_b_GT[cl]
        ret_ordered = sorted(cl_vul_dict.items(), key=operator.itemgetter(1), reverse=True)
        ret_list = [i[0] for i in ret_ordered]  # Descending-ordered list of vul. values for boundary nodes.
        R = len(rel_list)

        if R != 0:
            # Precision @ k
            num = 0
            den = 0
            precision_k = []
            for n in range(1, R + 1):
                if ret_list[n - 1] in set(rel_list):
                    num += 1
                den += 1
                precision_k.append(num / float(den))
            cl_vul_b_precision[cl] = precision_k
            avg_precision = sum(precision_k) / float(len(precision_k))
            print("avg_precision: "+str(avg_precision))
            cl_vul_b_avg_precision[cl] = avg_precision

    top_k = 15
    avg_precision_all = []

    with open(metrics_file, 'a') as f:
        for k in range(1, top_k + 1):
            l = []
            num = 0
            den = 0
            for cl in cl_vul_b_avg_precision:
                if len(cl_vul_b_precision[cl]) >= k:
                    num += cl_vul_b_precision[cl][k - 1]
                    den += 1
                else:
                    continue
            avg_precision = num / float(den + 0.000001)
            l.append(avg_precision)
            #             print('avg_precision at k=',k,avg_precision)
            f.write('avg_precision at k=' + ' ' + str(k) + ' ' + str(avg_precision) + '\n')
            avg_precision_all.append(avg_precision)

        mean_avg_precision = sum(avg_precision_all) / float(len(avg_precision_all))
        f.write('mean_avg_precision' + ' ' + str(mean_avg_precision) + '\n')


def community_metric_evaluation(d_vul_C, d_vul_C_GT):
    sorted_d_vul_C = sorted(d_vul_C.items(), key=operator.itemgetter(1), reverse = True)
    sorted_d_vul_C_GT = sorted(d_vul_C_GT.items(), key=operator.itemgetter(1), reverse = True)
    rel_list = [i[0] for i in sorted_d_vul_C_GT] # Ranked vul community list based infected boundary node count in community.
    ret_list = [i[0] for i in sorted_d_vul_C] # Ranked vul community list based on metric score
    top_k = len(d_vul_C_GT.keys())

    dcg = 0
    idcg = 0
    with open(metrics_file, 'a') as f:
        for k in range(1,top_k+1):
            rel_k = d_vul_C_GT[ret_list[k-1]]
            dcg += rel_k/float(math.log((k+1),2))
            i_rel_k = d_vul_C_GT[rel_list[k-1]]
            idcg += i_rel_k/float(math.log((k+1),2))

            ndcg = dcg/float(idcg)
#             print('For k=',k, ndcg)
            f.write('For k= ' +str(k) +' '+ str(ndcg)+'\n')


def community_metric_evaluation1(d_vul_C, d_vul_C_GT):
    sorted_d_vul_C = sorted(d_vul_C.items(), key=operator.itemgetter(1), reverse = True)
    sorted_d_vul_C_GT = sorted(d_vul_C_GT.items(), key=operator.itemgetter(1), reverse = True)
    rel_list = [i[0] for i in sorted_d_vul_C_GT] # Ranked vul community list based infected boundary node count in community.
    ret_list = [i[0] for i in sorted_d_vul_C] # Ranked vul community list based on metric score
    top_k = len(d_vul_C_GT.keys())

    with open(metrics_file, 'a') as f:
        for k in range(1, top_k + 1):
            tau, p_value = stats.kendalltau(rel_list, ret_list)
            f.write('For k= ' + str(k) + ' ' + str(tau)+'\n')

def generate_metrics(tweet_id):
    cd_list = ['louvain_', 'infomap_', 'label_propagation_']
    for cd in cd_list:
        cd_file = cd+ tweet_id+ '.txt'
#         cd_file = ''.join([cd, tweet_id, '.txt'])
        start_date = datetime.datetime.now()
        print(cd+" started: "+str(start_date))
        clusters = read_cd_file(cd_file, cd)
        d_vul_b, d_vul_b_GT, d_vul_C, d_vul_C_GT = run_algorithm(G, s_spreaders, clusters)
        boundary_metric_evaluation(d_vul_b, d_vul_b_GT, clusters)
        community_metric_evaluation1(d_vul_C, d_vul_C_GT)
        end_date = datetime.datetime.now()
        print(cd+" ended: "+str(end_date))
        print(cd+" duration: "+str((end_date - start_date).total_seconds()))

ended_program = datetime.datetime.now()
generate_metrics(tweet_id)
print('Program ended: '+str(ended_program))
print("Program Duration: "+str((ended_program - start_program).total_seconds()))